{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, persistent_workers=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2, persistent_workers=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(pl.LightningModule):\n",
    "    def __init__(self, in_channels = 3, patch_size = 8, embedding_dim = 128):\n",
    "        super().__init__()\n",
    "        self.unfolding = nn.Unfold(kernel_size = patch_size, stride = patch_size)\n",
    "        self.projection = nn.Linear(in_channels * patch_size ** 2 , embedding_dim)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        x = self.unfolding(x) # H * W * C -> N * ( P * P * C)\n",
    "        x = x.transpose(1, 2) # N * ( P * P * C) -> N * ( P * P * C)\n",
    "        x = self.projection(x) # N * ( P * P * C) -> N * E\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(pl.LightningModule):\n",
    "    def __init__(self, dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        \n",
    "        self.K = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.Q = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.V = nn.Linear(in_features=dim, out_features=dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(in_features=dim, out_features=dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        K = self.K(x)\n",
    "        Q = self.Q(x)\n",
    "        V = self.V(x)\n",
    "        \n",
    "        K = K.view(x.shape[0], x.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        Q = Q.view(x.shape[0], x.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(x.shape[0], x.shape[1], self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        attention = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "        x = torch.matmul(attention, V)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.dim)\n",
    "        x = self.out_proj(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, in_features, hidden_features, out_features, dropout=0.1, activation=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=hidden_features)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.activation = activation()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(pl.LightningModule):\n",
    "    def __init__(self, dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln_pre_attn = nn.LayerNorm(dim)\n",
    "        self.attention = MultiHeadAttention(dim, n_heads)\n",
    "        self.ln_pre_ffn = nn.LayerNorm(dim)\n",
    "        self.ffn = MLP(dim, dim * 4, dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln_pre_attn(x))\n",
    "        x = x + self.ffn(self.ln_pre_ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, in_channels = 3, patch_size = 8, embedding_dim = 128, n_blocks = 3 , n_heads = 8, out_dim = 10, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Patch Embeddings\n",
    "        self.patch_embeddings = PatchEmbeddings(in_channels = in_channels, patch_size = patch_size, embedding_dim = embedding_dim)\n",
    "        \n",
    "        # Positional Embeddings\n",
    "        num_patches = (32 // patch_size) ** 2\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, 1 + num_patches, embedding_dim))\n",
    "        \n",
    "        # Class Token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoder(embedding_dim, n_heads, dropout) for _ in range(n_blocks)])\n",
    "        \n",
    "        # MLP Head\n",
    "        self.mlp_head = MLP(embedding_dim, embedding_dim * 4, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = torch.cat([self.class_token.expand(B, -1, -1), x], dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n",
    "    \n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss, y_hat, y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss,y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('train_accuracy', acc, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_accuracy', acc, prog_bar=True, on_epoch=True)\n",
    "        return {'val_loss': loss, 'val_accuracy': acc}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._shared_step(batch, batch_idx)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_accuracy', acc, prog_bar=True)\n",
    "        return {'test_loss': loss, 'test_accuracy': acc}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name                | Type            | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | patch_embeddings    | PatchEmbeddings | 24.7 K | train\n",
      "1 | transformer_encoder | Sequential      | 594 K  | train\n",
      "2 | mlp_head            | MLP             | 71.2 K | train\n",
      "  | other params        | n/a             | 2.3 K  | n/a  \n",
      "----------------------------------------------------------------\n",
      "693 K     Trainable params\n",
      "0         Non-trainable params\n",
      "693 K     Total params\n",
      "2.772     Total estimated model params size (MB)\n",
      "48        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  78%|███████▊  | 245/313 [00:08<00:02, 30.46it/s, v_num=24, train_loss_step=1.630, train_accuracy_step=0.453]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=50,                  \n",
    "    precision=16,                   # Entraînement en demi-précision pour accélérer et réduire l'utilisation de la mémoire\n",
    "    gradient_clip_val=1.0,          # Clip les gradients pour stabiliser l'entraînement\n",
    "    enable_progress_bar=True,       # Affiche la barre de progression\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=trainloader, val_dataloaders=valloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
